@article{Brown2020,
   abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
   author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
   journal = {arXiv},
   month = {5},
   publisher = {arXiv},
   title = {Language Models are Few-Shot Learners},
   url = {http://arxiv.org/abs/2005.14165},
   year = {2020},
}
@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   journal = {arXiv},
   month = {1},
   publisher = {arXiv},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {http://arxiv.org/abs/1701.06538},
   year = {2017},
}
@article{Fedus2021,
   abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
   author = {William Fedus and Barret Zoph and Noam Shazeer},
   month = {1},
   title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
   url = {http://arxiv.org/abs/2101.03961},
   year = {2021},
}
@article{Hafner2020,
   abstract = {We introduce a unified objective for action and perception of intelligent agents. Extending representation learning and control, we minimize the joint divergence between the combined system of agent and environment and a target distribution. Intuitively, such agents use perception to align their beliefs with the world, and use actions to align the world with their beliefs. Minimizing the joint divergence to an expressive target maximizes the mutual information between the agent's representations and inputs, thus inferring representations that are informative of past inputs and exploring future inputs that are informative of the representations. This lets us explain intrinsic objectives, such as representation learning, information gain, empowerment, and skill discovery from minimal assumptions. Moreover, interpreting the target distribution as a latent variable model suggests powerful world models as a path toward highly adaptive agents that seek large niches in their environments, rendering task rewards optional. The framework provides a common language for comparing a wide range of objectives, advances the understanding of latent variables for decision making, and offers a recipe for designing novel objectives. We recommend deriving future agent objectives the joint divergence to facilitate comparison, to point out the agent's target distribution, and to identify the intrinsic objective terms needed to reach that distribution.},
   author = {Danijar Hafner and Pedro A. Ortega and Jimmy Ba and Thomas Parr and Karl Friston and Nicolas Heess},
   journal = {arXiv},
   month = {9},
   publisher = {arXiv},
   title = {Action and Perception as Divergence Minimization},
   url = {http://arxiv.org/abs/2009.01791},
   year = {2020},
}
@article{Gutknecht2020,
   abstract = {Partial information decomposition (PID) seeks to decompose the multivariate mutual information that a set of source variables contains about a target variable into basic pieces, the so called "atoms of information". Each atom describes a distinct way in which the sources may contain information about the target. In this paper we show, first, that the entire theory of partial information decomposition can be derived from considerations of elementary parthood relationships between information contributions. This way of approaching the problem has the advantage of directly characterizing the atoms of information, instead of taking an indirect approach via the concept of redundancy. Secondly, we describe several intriguing links between PID and formal logic. In particular, we show how to define a measure of PID based on the information provided by certain statements about source realizations. Furthermore, we show how the mathematical lattice structure underlying PID theory can be translated into an isomorphic structure of logical statements with a particularly simple ordering relation: logical implication. The conclusion to be drawn from these considerations is that there are three isomorphic "worlds" of partial information decomposition, i.e. three equivalent ways to mathematically describe the decomposition of the information carried by a set of sources about a target: the world of parthood relationships, the world of logical statements, and the world of antichains that was utilized by Williams and Beer in their original exposition of PID theory. We additionally show how the parthood perspective provides a systematic way to answer a type of question that has been much discussed in the PID field: whether a partial information decomposition can be uniquely determined based on concepts other than redundant information.},
   author = {Aaron J. Gutknecht and Michael Wibral and Abdullah Makkeh},
   journal = {arXiv},
   month = {8},
   publisher = {arXiv},
   title = {Bits and Pieces: Understanding Information Decomposition from Part-whole Relationships and Formal Logic},
   url = {http://arxiv.org/abs/2008.09535},
   year = {2020},
}
@article{Lee2019,
   abstract = {There are two main paradigms for brain-related science, with different implications for brain-focused intervention or advancement. The paradigm of homeostasis (“stability through constancy,” Walter Cannon), originating from laboratory-based experimental physiology pioneered by Claude Bernard, shows that living systems tend to maintain system functionality in the direction of constancy (or similitude). The aim of physiology is to elucidate the factors that maintain homeostasis, and therapeutics aim to correct abnormal factor functions. The homeostasis paradigm does not formally recognize influences outside its controlled experimental frames and it is variable in its modeling of neural contributions. The paradigm of allostatic orchestration (PAO) extends the principle of allostasis (“stability through change”) as originally put forth by Peter Sterling. The PAO originates from an evolutionary perspective and recognizes that biological set points change in anticipation of changing environments. The brain is the organ of central command, orchestrating cross-system operations to support optimal behavior at the level of the whole organism. Alternative views of blood pressure regulation and posttraumatic stress disorder (PTSD) illustrate differences between the paradigms. For the PAO, complexities of top-down neural effects and environmental context are foundational (not to be “factored out”), and anticipatory regulation is the principle of their interface. The allostatic state represents the integrated totality of brain-body interactions. Health itself is an allostatic state of optimal anticipatory oscillation, hypothesized to relate to the state of criticality, a mathematical point of poise between phases, on the border between order and disorder (or the “edge of chaos”). Diseases are allostatic states of impaired anticipatory oscillations, demonstrated as rigidifications of set points across the brain and body (disease comorbidity). Conciliation of the paradigms is possible, with “reactive homeostasis” resolved as an illusion stemming from the anticipation of environmental monotony. Considerations are presented with respect to implications of the two paradigms for brain-focused intervention or advancement; the hypothesis that the state of criticality is a vehicle for evolutionary processes; concordance with a philosophy of freedom based on ethical individualism as well as self-creativity, non-obsolescence, empowerment, and citizenship; and concluding reflections on the science and ethics of the placebo, and the potential for virtuous cycles of brain-Anthropocene interactions.},
   author = {Sung W. Lee},
   doi = {10.3389/fnhum.2019.00129},
   issn = {16625161},
   journal = {Frontiers in Human Neuroscience},
   keywords = {Allostasis,Blood pressure,Complexity,Criticality,Evolution,Homeostasis,Neuroethics,Posttraumatic stress disorder},
   month = {2},
   publisher = {Frontiers Media S.A.},
   title = {A copernican approach to brain advancement: The paradigm of allostatic orchestration},
   volume = {13},
   url = {/pmc/articles/PMC6499026/ /pmc/articles/PMC6499026/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6499026/},
   year = {2019},
}
@article{Ahissar2016,
   abstract = {Perception of external objects involves sensory acquisition via the relevant sensory organs. A widely-accepted assumption is that the sensory organ is the first station in a serial chain of processing circuits leading to an internal circuit in which a percept emerges. This open-loop scheme, in which the interaction between the sensory organ and the environment is not affected by its concurrent downstream neuronal processing, is strongly challenged by behavioral and anatomical data. We present here a hypothesis in which the perception of external objects is a closed-loop dynamical process encompassing loops that integrate the organism and its environment and converging towards organism-environment steady-states. We discuss the consistency of closed-loop perception (CLP) with empirical data and show that it can be synthesized in a robotic setup. Testable predictions are proposed for empirical distinction between open and closed loop schemes of perception.},
   author = {Ehud Ahissar and Eldad Assa},
   doi = {10.7554/eLife.12830},
   issn = {2050084X},
   issue = {MAY2016},
   journal = {eLife},
   month = {5},
   pmid = {27159238},
   publisher = {eLife Sciences Publications Ltd},
   title = {Perception as a closed-loop convergence process},
   volume = {5},
   year = {2016},
}
@article{Vergara2019,
   abstract = {A major goal of neuroscience is understanding how neurons arrange themselves into neural networks that result in behavior. Most theoretical and experimental efforts have focused on a top-down approach which seeks to identify neuronal correlates of behaviors. This has been accomplished by effectively mapping specific behaviors to distinct neural patterns, or by creating computational models that produce a desired behavioral outcome. Nonetheless, these approaches have only implicitly considered the fact that neural tissue, like any other physical system, is subjected to several restrictions and boundaries of operations. Here, we proposed a new, bottom-up conceptual paradigm: The Energy Homeostasis Principle, where the balance between energy income, expenditure, and availability are the key parameters in determining the dynamics of neuronal phenomena found from molecular to behavioral levels. Neurons display high energy consumption relative to other cells, with metabolic consumption of the brain representing 20% of the whole-body oxygen uptake, contrasting with this organ representing only 2% of the body weight. Also, neurons have specialized surrounding tissue providing the necessary energy which, in the case of the brain, is provided by astrocytes. Moreover, and unlike other cell types with high energy demands such as muscle cells, neurons have strict aerobic metabolism. These facts indicate that neurons are highly sensitive to energy limitations, with Gibb's free energy dictating the direction of all cellular metabolic processes. From this activity, the largest energy, by far, is expended by action potentials and post-synaptic potentials; therefore, plasticity can be reinterpreted in terms of their energy context. Consequently, neurons, through their synapses, impose energy demands over post-synaptic neurons in a close loop-manner, modulating the dynamics of local circuits. Subsequently, the energy dynamics end up impacting the homeostatic mechanisms of neuronal networks. Furthermore, local energy management also emerges as a neural population property, where most of the energy expenses are triggered by sensory or other modulatory inputs. Local energy management in neurons may be sufficient to explain the emergence of behavior, enabling the assessment of which properties arise in neural circuits and how. Essentially, the proposal of the Energy Homeostasis Principle is also readily testable for simple neuronal networks.},
   author = {Rodrigo C. Vergara and Sebastián Jaramillo-Riveri and Alejandro Luarte and Cristóbal Moënne-Loccoz and Rómulo Fuentes and Andrés Couve and Pedro E. Maldonado},
   doi = {10.3389/fncom.2019.00049},
   issn = {16625188},
   journal = {Frontiers in Computational Neuroscience},
   keywords = {behavior,emergent properties,energy,homeostasis,neuronal networks},
   month = {7},
   pages = {49},
   publisher = {Frontiers Media S.A.},
   title = {The Energy Homeostasis Principle: Neuronal Energy Regulation Drives Local Network Dynamics Generating Behavior},
   volume = {13},
   url = {/pmc/articles/PMC6664078/ /pmc/articles/PMC6664078/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6664078/},
   year = {2019},
}
@article{Zappetti2021,
   abstract = {Tensegrity structures are lightweight, can undergo large deformations, and have outstanding robustness capabilities. These unique properties inspired roboticists to investigate their use. However, the morphological design, control, assembly, and actuation of tensegrity robots are still difficult tasks. Moreover, the stiffness of tensegrity robots is still an underestimated design parameter. In this article, we propose to use easy to assemble, actuated tensegrity modules and body-brain co-evolution to design soft tensegrity modular robots. Moreover, we prove the importance of tensegrity robots stiffness showing how the evolution suggests a different morphology, control, and locomotion strategy according to the modules stiffness.},
   author = {Davide Zappetti and Jean Marc Bejjani and Dario Floreano},
   keywords = {Soft modular robots,evolutionary design,programmable stiffness,tensegrity robots},
   month = {1},
   title = {Evolutionary Co-Design of Morphology and Control of Soft Tensegrity Modular Robots with Programmable Stiffness},
   url = {http://arxiv.org/abs/2101.11772},
   year = {2021},
}
@web_page{,
   title = {Entropy and life - Wikipedia},
   url = {https://en.wikipedia.org/wiki/Entropy_and_life},
}
@book{Schneider2005,
   author = {Eric D. Schneider and Dorion Sagan},
   city = {Chicago, United States},
   pages = {15},
   publisher = {The University of Chicago Press},
   title = {Into the Cool: Energy Flow Thermodynamics and Life},
   year = {2005},
}
@article{Struzik1987,
   abstract = {The generalized Carnot principle introduced by Brillouin provides a link between negentropy and information, but does not take into account the information stored in the brain, which is clearly excluded by the author. Further step in the generalization of Carnot's principle, which includes information turnover in the brain was accomplished by Kepiński in his theory of the metabolism of energy and information. According to Schrödinger, life processes require a supply of negentropy rather than energy stored in the food, more precisely, e.g., in glucose utilized by the brain, since energy is conserved, whereas negentropy is dissipated. The information (communication) channel transmits maximum information when the band width of the transmitted frequency is limited, and much less information when the restriction concerns the transmitted power, or energy. This can explain a considerable decrease in the information metabolism observed in depressive patients, whose life dynamics and, consequently, the amount of energy available for information metabolism is severely lowered. Thus, the fall in information metabolism is more pronounced in depression than in not too late phases of schizophrenia. As appears from Fonberg's studies the amygdaloid nucleus is responsible for the life dynamics. in different types of neuron code change is a significant parameter, which was so strongly stressed by Kepiński. Other problems discussed in the paper include: selection of information and its structural localization, localization of particular phases of information metabolism and their phylogenetic significance. © 1987 Informa UK Ltd All rights reserved: reproduction in whole or part not permitted.},
   author = {Tadeusz Struzik},
   doi = {10.3109/00207458709002144},
   issn = {00207454},
   issue = {1-2},
   journal = {International Journal of Neuroscience},
   keywords = {Carnot's principle,Information theory,Kepiński's metabolism},
   pages = {105-111},
   pmid = {3654085},
   publisher = {Informa Healthcare},
   title = {Kelpiński's information metabolism, carnot's principle and information theory},
   volume = {36},
   year = {1987},
}
@article{Hirsh2012,
   abstract = {Entropy, a concept derived from thermodynamics and information theory, describes the amount of uncertainty and disorder within a system. Self-organizing systems engage in a continual dialogue with the environment and must adapt themselves to changing circumstances to keep internal entropy at a manageable level. We propose the entropy model of uncertainty (EMU), an integrative theoretical framework that applies the idea of entropy to the human information system to understand uncertainty-related anxiety. Four major tenets of EMU are proposed: (a) Uncertainty poses a critical adaptive challenge for any organism, so individuals are motivated to keep it at a manageable level; (b) uncertainty emerges as a function of the conflict between competing perceptual and behavioral affordances; (c) adopting clear goals and belief structures helps to constrain the experience of uncertainty by reducing the spread of competing affordances; and (d) uncertainty is experienced subjectively as anxiety and is associated with activity in the anterior cingulate cortex and with heightened noradrenaline release. By placing the discussion of uncertainty management, a fundamental biological necessity, within the framework of information theory and self-organizing systems, our model helps to situate key psychological processes within a broader physical, conceptual, and evolutionary context. © 2012 American Psychological Association.},
   author = {Jacob B. Hirsh and Raymond A. Mar and Jordan B. Peterson},
   doi = {10.1037/a0026767},
   issn = {0033295X},
   issue = {2},
   journal = {Psychological Review},
   keywords = {Anxiety,Behavioral inhibition,Entropy,Self-organization,Uncertainty},
   pages = {304-320},
   pmid = {22250757},
   title = {Psychological entropy: A framework for understanding uncertainty-related anxiety},
   volume = {119},
   url = {https://www.researchgate.net/publication/221752816},
   year = {2012},
}
@article{Friston2007,
   abstract = {If one formulates Helmholtz's ideas about perception in terms of modern-day theories one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. Using constructs from statistical physics it can be shown that the problems of inferring what cause our sensory inputs and learning causal regularities in the sensorium can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory information is generated. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of the brain's organisation and responses. In this paper, we suggest that these perceptual processes are just one emergent property of systems that conform to a free-energy principle. The free-energy considered here represents a bound on the surprise inherent in any exchange with the environment, under expectations encoded by its state or configuration. A system can minimise free-energy by changing its configuration to change the way it samples the environment, or to change its expectations. These changes correspond to action and perception, respectively, and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment implies that the system's state and structure encode an implicit and probabilistic model of the environment. We will look at models entailed by the brain and how minimisation of free-energy can explain its dynamics and structure. © 2007 Springer Science+Business Media B.V.},
   author = {Karl J. Friston and Klaas E. Stephan},
   doi = {10.1007/s11229-007-9237-y},
   issn = {00397857},
   issue = {3},
   journal = {Synthese},
   keywords = {Action,Attention,Free-energy,Hierarchical,Inference,Learning,Perception,Selection,Value,Variational Bayes},
   month = {12},
   pages = {417-458},
   publisher = {Europe PMC Funders},
   title = {Free-energy and the brain},
   volume = {159},
   url = {/pmc/articles/PMC2660582/ /pmc/articles/PMC2660582/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660582/},
   year = {2007},
}
@report{,
   abstract = {Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environ-ment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution .},
   author = {Donald J Hejna Iii and Pieter Abbeel and Lerrel Pinto},
   title = {TASK-AGNOSTIC MORPHOLOGY EVOLUTION},
   url = {https://sites.google.com/view/task-agnostic-evolution},
}
@report{,
   abstract = {The integration of reasoning, learning, and decision-making is key to build more general AI systems. As a step in this direction, we propose a novel neural-logic architecture that can solve both inductive logic programming (ILP) and deep reinforcement learning (RL) problems. Our architecture defines a restricted but expressive continuous space of first-order logic programs by assigning weights to predicates instead of rules. Therefore, it is fully differentiable and can be efficiently trained with gradient descent. Besides, in the deep RL setting with actor-critic algorithms, we propose a novel efficient critic architecture. Compared to state-of-the-art methods on both ILP and RL problems, our proposition achieves excellent performance, while being able to provide a fully interpretable solution and scaling much better, especially during the testing phase.},
   author = {Matthieu Zimmer and Xuening Feng and Claire Glanois and Zhaohui Jiang and Jianyi Zhang and Paul Weng and Jianye Hao and Dong Li and Wulong Liu},
   title = {Differentiable Logic Machines},
}
@article{Shu2021,
   abstract = {For machine agents to successfully interact with humans in real-world settings, they will need to develop an understanding of human mental life. Intuitive psychology, the ability to reason about hidden mental variables that drive observable actions, comes naturally to people: even pre-verbal infants can tell agents from objects, expecting agents to act efficiently to achieve goals given constraints. Despite recent interest in machine agents that reason about other agents, it is not clear if such agents learn or hold the core psychology principles that drive human reasoning. Inspired by cognitive development studies on intuitive psychology, we present a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology. We validate AGENT with human-ratings, propose an evaluation protocol emphasizing generalization, and compare two strong baselines built on Bayesian inverse planning and a Theory of Mind neural network. Our results suggest that to pass the designed tests of core intuitive psychology at human levels, a model must acquire or have built-in representations of how agents plan, combining utility computations and core knowledge of objects and physics.},
   author = {Tianmin Shu and Abhishek Bhandwaldar and Chuang Gan and Kevin A. Smith and Shari Liu and Dan Gutfreund and Elizabeth Spelke and Joshua B. Tenenbaum and Tomer D. Ullman},
   month = {2},
   title = {AGENT: A Benchmark for Core Psychological Reasoning},
   url = {http://arxiv.org/abs/2102.12321},
   year = {2021},
}
