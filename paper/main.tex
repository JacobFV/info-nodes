\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, square}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

%\usepackage[square,numbers]{natbib}
% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\input{preamble.sty}

\title{Predictive General Intelligence}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jacob F. Valdez \\
  Limboid AI \\
  \texttt{jacob.valdez@limboid.ai}
}

\begin{document}

\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}

\I should be clear: \my objective is not ``can \I make artificial general intelligence?'' but ``how general can \I make artificial intelligence''. The strictest definition of ``general'' intelligence is intractable; for every pattern recognizer, there exists a pattern it cannot recognize. (CITE AGI paper on my iPad) Nonetheless, natural and artificial approaches to the challange informally provide us an intuition for the illusion and, more importantly, direction to follow.

It must also be ackowledged that the universe is \textit{not} a white noise data genorator. Instead, once tamed the scientific mind percieves elementary and emergent isomorpisms, YYYY's, and ZZZ's (FIND TWO DIFFERENT WORDS FOR ISOMORPHISMS, PATTERNS (REUSED), ETC.) interacting in an orderly and predictable manner. The information system required to understand these patterns may not require so much data but rather, a few well chosen priors that are reasonably aligned with the general distribution of data.
    
This work examines some of the principles underlying intelligence and consolidates them into a working implementation of task-agnostic, open-ended artificial `general' intelligence together with experiments and ablation studies. \My paper is organized as follows: section 2 reviews key principles of intelligence; section 3 describes their novel composition: Predictive General Intelligence (PGI); section 4 presents experiments with observations over a diverse set of open-ended learning environments and numerous close-ended tasks and benchmarks and also describes ablation studies; section 5 presents descriptive and comparative analyses of individual experimental results; and finally, section 6 gives a general discussion of this work along with its broader impact and a conclusion.  

\section{Principles of Intelligence}

Intelligence has been defined as ``an agent’s ability to achieve goals in a wide range of environments.'' (Legg and Hutter, 2007 CITE) and ``skill-acquisition efficiency'' with respect to available information (Chollet 2019 CITE). While its mode of expression varies between and within natural and artificial settings, \I extract overarching principles and comparisons in this section. \My aim is not to exhaust every thought and theory but only to provide a background for introducing PGI. See CITE for a more extensive discussion.

\subsection{Energetic grounding}
Intelligence begins with information (CITE) which, in turn, depends on energy. Under any probability distribution $p$, information theory even equates information with energy by $E(x) = - \log{p(x)} $ (CITE). With this negative log-likelihood relationship, it is easy to see that unexpected events are therefore energetic ones. For instance, signaling with prior-optimized codebook, the cross entropy of a signal directly relates both electrical energy consumed and information transmitted. (CITE) Likewise EEG's are used to approximate the cognitive involvement of a brain region by measuring its glucose metabolism. (CITE)

In all natural settings, free energy minimization is the norm, and its increase is an exception: objects descend potential wells; virtual particles dissapate; the princple of least action obtains a minimal route for system evolution. In turn, decrease of free energy -- that available for work and maintenance of order -- produces increase in entropy: structured arangements evaporate; gas pressures equalize; wavefunctions spread out. Notably, the Casmir force directly attracts of repels matter apart from any of the four fundamental forces such that expected energy homogenizes. 

Life stands in defiance of this entropic trend, yet even in the struggle for survival, living systems continually optimize energy expendature. By the energy-information relationship, this means minimal information transfer and greatest potential for intelligence. For example, homeostatic mechanisms work to equalize energy production and consumption. This results in minimal free energy, optimal energy expendature, and ``survival intelligence'' (CITE). Genetic code likewise gives the muscular-skeletal system innate ``mechanical intelligence'' which `ofloads' some of the locomotive learning that a vertabrae's brain must perform. (Davide Zappetti CITE but that article is about robots not natural systems). In general, natural living systems represent a prior expectation over their environmental state. It follows that as actual and expected environments diverge, survival imposes a greator information and energetic tax to mainten order. Again, probability expectation maximization accompanies energy minimization, hence control over the environment and intelligence.

It should be noted that intelligently meeting the energy challange is not simply about on storing away as much energy as possible, but equalizing energy intake and expenderture. Sustained positive free energy is just debilitating as negative extremes. \footnote{It should be noted in mobile life forms that selection pressures favor this extreme over the other. However excessive increase eventually accompanies loss of agility, hence suceptibility to prey. Additionally, as this is not a natural stressor, most animals are not equipted with the mechanisms to intelligently handle high amounts of free energy and suffer from resulting stress. For instance, chronic elevated levels of mobilized energy and its indicators such as blood sugar, free fatty acids, cortisol, and blood pressure are associated with inflammation, diabetes, immunosuppression, ulceration, corinary heart disease, and hypertension.} The brain exemplifies this principle. Consuming 20\% of the average human's (CHECK) resting metabolic energy in an organ only 1.5 kg. (3 lbs.) (CHECK), its neurons must balance a lean energy budget. Expensive, energetic signaling between neurons -- especially long internetwork axons -- kept to a bare minimum. The little energy available is used to maintain the neurons' delicate membrane and . Without a steady flow of energy, the brain quickly dies. However the opposite is equally dangerous. Unchecked, hyperglyemia leads to neuroinflammation and ultimately, cell death. To cope with energy stress in either direction, neurons generally remain very sensitive to their extracellular environment. If energy is short, cell growth pauses, synaptic thresholds increase, and synaptic junctions become more resistive. On the other hand, 

        
Neurons individually adapt to minimize the difference between produced and consumed energy. During glucose deprivation, cell growth and maintenance is paused, synaptic connection strengths are decreased, and signal thresholds are increased. Once free energy levels neutralize again, anabolic processes resume,  and the neuron becomes more likely to participate in large intranetwork conversations. Repeated expression of this local free energy minimization principle produces adaptive behavior -- intelligence. (see Figure 4 from ncbi.nlm.nih.gov/pmc/articles/PMC6664078) Even in the abscence of a reward system or natural body, sections of cortical tissue are experimentally observed to learn robotic vehicle control. (The Energy Homeostasis Principle: Neuronal Energy Regulation Drives Local Network Dynamics Generating Behavior) 

It comes as no surprise therefore that estimated cost minimization is the norm in behavioral psychology and emergant sciences: humans continually look for ways to increase their efficency or otherwise minimize cognitive and physical workload (MAKE SURE THIS IS A GOOD DEFINITION OF COST ESTIMATION); the global economy likewise . . .; and these principles extend directly to the research and development of AI. Human cognitive and financial energy form selective pressures to artificial intelligence survival.

The lesson for AI is to keep energy demands in foremost at all times. Obviously, this means minimizing the cross entropy or Kullbach  Leblief divergence of a neural network against its target data distribution, but energy minimization does not stop there. The actual algorithmic implementations of intelligence should be made as resource efficent as possible. As quoted earlier, intelligence has been defined as ``skill acquizition efficency TODO. While state of the art machine learning systems have displayed superhuman performance on a variety of problem domains, this is only achieved by massive amounts of physical energy and information. Comparing its 343TJ, 543 million dollar training budjet with the XX J of energy a YY takes to learn to speak English, GPT-3 seems quite unintelligent (









With energy or information is equated to the negative log likelihood of an event occurring, events that are more probable demand less energy while those that are less likely demand more energy. It is important to note: this is an allocentric measure. The information carried by a signal $x_i$ may be very useful to another human, yet $- \log{p(x_i)}$ only measures the information content of a signal with respect to a particular probability distribution $p$. This probability distribution may represent various information systems. For instance, in discrete signal communication, $p$ represents a prior expectation of signal occurrence. Optimized codebooks use short strings to identify frequently occurring events while longer stings discriminate between infrequently occurring ones. The effect is, transmitting codes for likely occurrences consumes less physical energy while the opposite is true for more informative, less expected signals. In quantum physics, a particle position in space may be represented by a probability wavefunction. Natural living systems represent a prior expectation over their environmental state. 
This is especially apparent under electroencephalogram observation of the brain where increased neuronal glucose metabolism is correlated with less expected signals from outside or inside the brain. 

In the context of machine learning, the model represents a probability distribution over input-output pairs. Training the model on a dataset of high-energy, novel input-output pairs is analogous to allowing a cold object to absorb some of the entropic energy from a warmer mass. On the other hand, the more likely an input-output pair is to be produced by a model, the more ‘neutral’ its temperature becomes and less informative it is to the model. This explains the traditional log improvement of training with respect to the amount of information processed. (SHOW FIGURE) Continuing the thermodynamics parallel, machine learning systems even share properties of states and transitions during training. (figure XX. CITE FIGURE SHOWING SOLID-LIQUID-GAS AND NEURAL NETWORK TRAINING)

When its expectation is taken over the event space, probabilistic energy becomes entropy $H(p) = \expectation{-\log{p(x)}}$. The entropy of a probability distribution $q$ with respect to a prior expectation $p$ gives the cross entropy $H(p,q) = \expectation[p]{ \log{ q(x) } }$ and asymmetric energy loss from one distribution to another may be defined by the Kullback-Leibler Divergence metric $\kld{p}{q} = H(p,q) - H(p) = \expectation[p]{ \log {\frac {p(x)}{q(x)} } }$. Kullback-Leibler Divergence gives an objective measure to the minimum information required to transform a prior to a posterior, and if we let the uniform prior $\mathcal{U}$ represent zero information, the Kullback-Leibler divergence of any other distribution $q$ from it $\kld{q}{\mathcal{U}}$ then gives an absolute measure to the information contained in $q$. 

These mathematically pure yet rich ideas from information theory, give basis to formalize connections across numerous domains of science and identify the overarching motif of free energy minimization. With respect to any particular system, free energy is the complement of bound and entropic energy. For instance, note the similarity between Gibbs free energy $G$, Helmholtz free energy $F$, grand free energy $\phi$, and Kullback-Leibler divergence $\kld{p}{q}$:
\begin{alignat*}{4}
& G    && \equiv && {} E + PV \quad && - TS \\
& F    && \equiv && {} E      \quad && - TS \\
& \phi && \equiv && {} E      \quad && - TS - \mu N \\
& \underbrace{\kld{p}{q}}_{\text{free energy}} && \equiv && {} \underbrace{H(p,q)}_{\text{bound energy}} \quad && \underbrace{- H(p)}_{\text{entropy}}
\end{alignat*}
In all domains of definition, the universe fights free energy. This involves entropy maximization. For instance, the Casmir effect provides an attractive or repulsive force without invoking any of the fundamental forces. Instead it influences particles in a way that directly maximizes the uniformity and entropy of space. The principle of least action states that XXXX.

Life represents a defiance of this overall trend, yet even in its struggle, organisms do not expend unnecessary amounts of energy. Blood sugar is minimized until necessary. YYY. Allostatic control theory shows that life cannot tolerate excessive amounts of energy and such unusual physiological states as those fostered by modern society give rise to diabetes, chronic hypertension, and other lifestyle problems.

In the mind, Friston’s free energy principle posits that 
- define
- discuss examples of action in humans
- discuss neuronal principle and experiment of neural controlled car in “Free Energy and the Brain”
- explain that the EEG is observing areas of extra glucose consumption - aka free energy



INTRODUCE INFORMATION THEORY HERE. THEN DISCUSS HOW LIFE IS A DEFIANCE OF THE TREND. THEN HOW LIFE EATS, RESTS, AND ACTS. FINALLY MAKE PARALLELS WITH ARTIFICIAL INTELLIGENCE. 

Natural intelligence does not need to be spoon fed data to acquire Diverse, adapted skills and behaviors. This should not be the case for artificial intelligence either. 

The theoretical world of machine learning meets a hard reality when training time comes. State of the art AI systems consume thousands of RANT ABOUT FINANCIAL AND ENVIRONMENTAL ASPECTS OF LEARNING. In contrast, human intelligence finds relative ease in acquiring new skills, and the carbon footprint of physiologically comparable species does not approach computing systems. (CITE) 

How does natural intelligence cope with the reality of bioenergetics? By keeping it foremost in mind. Energetic demands provide strong guidance to the

Allostatic control theory IS THAT THE RIGHT NAME FOR THIS THEORY?        carries this thought further with “predictive regulation” which “proposes that efficient regulation requires anticipating needs and preparing to satisfy them before they arise.” (Allostasis: A model of predictive regulation CITE) DISCUSS PRINCIPLES OF ALLOSTATICS PAPER.

After feeding, the animal typically enters a ‘rest and digest’ period where a subset of its peripheral nervous system -- the parasympathetic system -- dominates decreasing respiration, heart rate, and blood pressure. Through these are natural processes promoting adaptive, diverse intelligent animal life, importantly, they are not appropriate at all times. In excess, they can reduce the amount of mobile energy available for survival functions. Homeostatic control therefore must balance energy collection against mobilization.
 
data = increase budget
mobile energy = budget
information energy = decrease budget

I might be confusing information, data, and energy
We extend these bioenergetic themes to intelligence with information theory’s definition of MOVE THIS TO BIOLOGICAL PART BEFORE INTORDUCING 

This brief yet rich consideration of information

We emphasize that data only loses expected energy with consumption. That is, for any machine learning model $f \colon X \mapsto Y$ where $X$ and $Y$ are the domains of inputs and outputs respectively, the probabilistic energy of the output is upper-bounded by the input $E({X}) \ge E({Y})$. Using deep learning, we might imagine each layer of the neural network as ‘absorbing’ some of the signal’s energy as it maps the signal onward $E(X) \ge E(H^1) \ge E({H}^2) \ge E(H^l)$. In the specific framework of agent-environment interaction, this means following the route from perception to action. 

As with physical energy, information theoretic energy -- information -- is always readily available. However, its relative availability may not be constant and the form it exists in may not be useful to the agent. Increasingly adapted and general intelligences should be able to anticipate demand and consume information in a variety of forms. This implies the agent should receive diverse modalities of information.

Just as stored information is analogous to energy, data consumption is to eating. Collecting training data gives the AI system information which may lower the negative log likelihood of future data. Regarding the ‘nutrition’ of consumed data, recent works have drawn attention to the danger of poisoning -- injecting data into a training pipeline that decreases its performance or stability -- as well as XXXX -- which is comparable to malnutrition in our discussion. Most existing machine learning systems today are embryos in this respect. They are force fed data with little attention to feedback. In contrast, as animal life develops, it gains the capability to reject food that is presented to it, choose food from a range of options, and even independently search out food without the intervention of a trainer. AI systems should imitate natural intelligence accordingly.

The process of training may be likened to digestion. JUSTIFY. EXPLAIN.

Finally, using information theoretic energy allows us to define a reward system carrying rich implications from nature. NATURAL REWARD SIGNALS ARE HIGHLY CORRELATED WITH ENERGY ABUNDANCE. DISCUSS HOW NATURAL REWARD SYSTEMS
- arise from a complex, chaotic “internal milieu” thus shaking out local minima
- resist tampering. They break down in most cases to not forever favor ‘local behavioral optima’
- are intrinsic. So while the AI system is extrinsically reward-free, it generates its own intrinsic reward signals by information theoretic mechanisms

Triply grounding the AI system’s energy in its compute platform, the interaction environment, and the AI's information theoretic energy helps align its objectives with our own.
Open-ended learning (combine with previous paragraph)
Few points deserve as much emphasis as reward-free (REMOVE), task-agnostic, open-ended learning in the domain of abstract principles of intelligence. Supervised learning plays a key role in superhuman narrow AI systems today, and supervised learning over large datasets - notably in language modeling - extracts information useful to tasks in unexpected ways (CITE GPT3 \& Switch-Transformer papers). Nonetheless, breaking present intelligence generality barriers demands zooming out from mean absolute errors, Inception scores, BLEU scores, IoU’s - completely leaving the domain of classic supervised learning - to entropy, Kullback–Leibler divergence, and other ‘pure’ information-theoretic metrics.

Despite the advantages these principles of intelligence bring to an AI system, more inductive biases are necessary to approach human-like cognition and behavior (See Section 5 “Ablations”).
Agent-Environment Interaction
It should be clear that imitating natural intelligence demands adopting an agent-environment interaction framework. Unlike blind machine learning pipelines, the AI system must be able to perceive its actions and their consequences just as it would take in traditional observations. 

However this does not necessarily limit the agent to a single body. Parallelization

The real world, of course, is not stable. The paradigm of allostaic orchestration therefore posits organisms to ``maintain stability through change'' with anticipatory rather than reactive behavior. 
Each of these principles of intelligence provide unique advantages to AI systems, and proofs-of-concept such as RAINBOW reinforcement learning, X, and Y demonstrate that combining multiple inductive biases into one AI system synergizes the advantages of individual ideas. However, to our knowledge, no work has combined all of them in one system. Our work, Predictive ‘General’ Intelligence, does this.

\section{Predictive ‘General’ Intelligence}

Make brief statement on why prediction is a general principle of intelligence

\section{Experiments}

\section{Analyses}

\section{Discussion}



\section*{Broader Impact}

Authors are required to include a statement of the broader impact of their work, including its ethical aspects and future societal consequences. 
Authors should discuss both positive and negative outcomes, if any. For instance, authors should discuss a) 
who may benefit from this research, b) who may be put at disadvantage from this research, c) what are the consequences of failure of the system, and d) whether the task/method leverages
biases in the data. If authors believe this is not applicable to them, authors can simply state this.

Use unnumbered first level headings for this section, which should go at the end of the paper. {\bf Note that this section does not count towards the eight pages of content that are allowed.}

\begin{ack}
We thank Deokgun Park from the University of Texas at Arlington for assisting in the acedemic publishing process. \citep[p. 130]{Ful83}

This work has no financial incentives and is personally financed.
\end{ack}

\medskip

\bibliographystyle{unsrtnat}
\bibliography{bibliography}

\end{document}
